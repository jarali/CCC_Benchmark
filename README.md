# Confidential Cloud Computing AI/ML Benchmark

## What is this?
This is a Confidential Cloud Computing AI/ML Benchmark which was created as part of a scientific paper.
You can read and access the paper by opening the Paper folder.
Paper_Graphs contains all the graphs which are created using the data collected as part of the paper.
Paper_Result contains all the raw data which was generated by running the benchmarks on the machines mentioned in the paper.

## Motivation: Why was this created?
This benchmark was created to recreate and benchmark or compare the potency of a system when it comes to ML or AI workloads. Since a lot of research focusing on the performance difference between non-confidential and confidential computers uses synthetic benchmarks this tool aims to fill that gap and provide a measurement that researchers and professionals can use to determine the feasibility and performance impact of using confidential cloud computing to execute their workload.

## How does it work?
This repository uses Kaggle notebooks in addition to Fio and PyPerformance to execute a machine learning or data science focused benchmark.
The Sequence of which benchmarks, how long the waittime between them, how many times and in what order they should be executed are in the benchmark_plan.txt file.
The Datasets are downloaded from Kaggle by using your own login data from the Secrets.txt this is done, so GitHub does not have to host the files, which would be impossible because of file size restrictions.

## Prerequisites:
- Linux Machine (for executing the benchmark)  
_While it is possible to execute the benchmark under windows, it is not recommended, because of the file structure._
- Kaggle Account (for downloading the datasets)
- AVX Capable Machine (ideally)

## How To: Running the Benchmark
1. **Install git (if not installed) and get repo**  
```bash
sudo apt update
sudo apt install git
git clone https://www.github.com/jarali/CCC_Benchmark
```

2. **Use nano (or any editor) to update credentials in ChangeMeSecrets.txt and save as Secrets.txt**  
```bash
nano ChangeMeSecrets.txt
```

3. **Install curl (if not installed) and execute script**  
```bash
sudo apt install curl
bash setup_virtual_env.sh
```

   - 3.1 **If needed, because python was not installed properly with pyenv refresh terminal and execute again**  
   ```bash
   source ~/.bashrc
   bash setup_virtual_env.sh
   ```

4. **Activate enviroment (if not already active) and execute:**  
```bash
pyenv activate benchmark-environment
pip install nbformat nbconvert psutil
```

5. **Prepare the benchmark by running:** 
```bash 
python manage.py -init
```

6. **Execute the benchmark by running:**  
```bash
python manage.py -bench --name Test-TDX-Encrypt
```

## Accessing the Results
The results of the benchmark are in the results folder. All benchmarks store results as a .txt file.
You can render graphs as png files by running:
```bash
python manage.py -render
```
or if you want svg files you can pass the svg flag:
```bash
python manage.py -render --svg
```
Of course, you can also always read the raw .txt files, if you want more details.

## Additional information
- The notebooks in this repository are not original and were changed because of complexity (runtime), compatibility or other factors. This means that some of the benchmarks and predictions might not perform as good as in the original. Also because of the structure of this repository with the separate datasets folder, the scripts needed to be updated.
- Some of the benchmarks require the AVX CPU Instruction set for them to work correctly, this is because keras and tensorflow but also umap require AVX to run. If your processor is not capable of running AVX (you can check by running `grep -m1 'avx' /proc/cpuinfo`) you can use the alternative, fast benchmark plan (fast_bench_plan.txt) by renaming it or swapping its contents with the benchmark_plan.txt
- In addition to the PyPerformance default benchmark you can either run:
   - pyperformance math
   - pyperformance regex
   - pyperformance serialize
- You can also run other fio benchmarks by modifying the benchmark plan for example:
   - fio_256k_randomwrite,standard,,10,1
- Generally there are 4 types of instructions the benchmark_plan.txt can accept/execute:
    - Kaggle Notebooks with datasets
    - The 4 predefined PyPerformance Benchmarks
    - The Systeminfo generation
    - Fio Benchmarks
-  If you're unsure on what machine the benchmark was run after running it or combining files from multiple machines, check the systeminfo file, which is created by default at the beginning of every run. 
- The uncompressed datasets are around 7.0GB when downloaded.
